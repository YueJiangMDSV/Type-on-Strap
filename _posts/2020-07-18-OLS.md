---
layout: post
title: Simple Introduction to OLS
tags: [Blog, Archive]
excerpt_separator: <!--more-->
---

Today, in this blog post, I want to talk about a very useful regression method: Ordinary Least Squares (OLS). It is frequently used in economics, statistics, and data science. It is simple to use, but it is also the foundation of some of the more advanced regression methods. 
<!--more-->
So you may ask: what is ordinary least squares? What is a regression? And why am I learning this? What am I gonna use it for? Just bear with me, and I will provide examples of why regressions are very useful in today’s world. Also, because this blog is just an elementary introduction for ordinary least squares, so I will not dive into the difficult math equations to prove its assumptions and merits. Nor will I go over specific codes to calculate it. It is beyond the scope of this blog.

## What is regression?
You can find a very technical and complicated explanation on Wikipedia: [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)$$^1$$. 
Let me use a simple example to illustrate it. We often see in crime/detective movies, when the detectives see the footprints of a suspect, they can instantly tell roughly how tall the suspect is. How did they do that? Is that some kind of magic? Of course not! They know that because there is a relationship between a person’s height and foot size. According to a study by the University of Rhode Island Department of Electrical, Computer and Biomedical Engineering, the normal height-to-foot ratio is about 6.6:1$$^2$$.
    
In layman's terms, regression analysis is a method to study the relationship between variables. It can be the relationship between a person’s height and foot size (like the example above), or the relationship between the amount of sunlight and corn production, or the relationship between the amount of cheese your mom put in your mac & cheese and how delicious it is. Just like people are connected with different relationships, variables can also be connected with certain relationships. Regression analysis helps us discover these relationships. 
     
There are two important concepts in regression: 
1. Dependent variable, or often called response variable. It is the target that you want to study. You want to study the factors that affect the variable and even to predict the behavior of this variable. 
2. Independent variables, or often called explanatory variables. As the name suggests, these variables are the factors that explain the changes of the dependent variable. 
   
The key of regression analysis is to identify the right dependent variable, the right independent variables that impact the dependent variable, and the right way to illustrate this relationship. In the real world setting, there could be various kinds of relationships, such as linear, quadratic, polynomial, exponential, logarithmic, and other non-linear types. Here I will only introduce ordinary least squares, which is the basic linear regression form. 

## What is OLS?

Again, you can refer to Wikipedia for a technical definition: [Ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)$$^3$$.
    
In order for OLS to work properly, there are several key components:
   
### Linear
the relationship between a dependent variable and independent variables must be linear. Below is an example of a linear relationship$$^4$$

![]({{ "/assets/img/post_img/linear-2.png" | relative_url}})
   
We can see that as `x` increases, `y` aslo increases, and their relationship follows a somewhat straight line. 

An example of non-linear relationship can be seen from the graph below:

![]({{ "/assets/img/post_img/non-linear.png" | relative_url}})

In the graph above, we can see that first as `x` increases, `y` decreases. But then as `x` continues to increase, `y` increases. This is clearly not a linear relationship. It could be quadratic, but we cannot say for sure, since this is only part of the data. If the relationship is quadratic or in other forms, then we cannot use OLS. It would do a bad job estimating the relationship.

Thus, to apply OLS, you have to make sure the relationship between the response variable and the explanatory variables is linear. Then the OLS relation can be defined use this equation:

$$y=\beta_0+\beta_1x$$

Does this remind you of something similar? That's right! It looks just like the equation of a line. I think most of you have encountered such a function during high school. Back then, you probably did a lot of calculations to figure out the slopes and the intercepts of this kind of function. And this is what we are doing now! OLS is just a more complicated way to figure out the slopes and the intercepts. Behind this equation, mathematicians and statisticians did a lot of hard work to figure out how OLS works to calculate $$\beta_0$$ and $$\beta_1$$, and they also proved that why OLS does a very good job estimating $$\beta_0$$ and $$\beta_1$$.

The equation above only includes one explanatory variable, but in theory, we can add as many as we like. However, adding more variables does not always make the regression better. There are trade-offs and the details are beyond the scope of this post. 

### Minimizing the sum of the squares of the differences.
The key for OLS to work so well in dipicitng the relationship between `x` and `y` is because it minimizes the sum of the squares of the differences. 

First, the differences. What differences?     

Let’s see the graph below. We can see in the graph that there is a line $$f(x_i)$$ almost simulates the pattern of $$y_1, y_2, …, y_10$$, but each $$y_i$$ is not exactly on the line. Here, the $$\{y_i\}$$ represents the true values, values that you get from a dataset. And the line $$f(x_i)$$ is the predicted value of $$\{y_i\}$$. It is a line that you think represents the relationship between $$x_i$$ and $$y_i$$. But how do we know this line is good? What does this line have to do with OLS?   

![]({{ "/assets/img/post_img/ols.png" | relative_url}})

      
Here, the differences are the differences between each $$y_i$$ and each $$f(x_i)$$, namely, the differences between each true value and each predicted value. To check how accurately each $$f(x_i)$$ predicts each $$y_i$$, we need to sum up all the differences. But some of the $$y_i$$ is larger than $$f(x_i)$$, some smaller. The positive and negative differences may cancel each other out. What shall we do then? To solve this problem, OLS first squares the differences, and then sums up the squared differences together. In this way, there are no negative differences to affect our analysis. 


## References:
1. Regression analysis. Wikipedia. (n.d.). Retrieved from https://en.wikipedia.org/wiki/Regression_analysis  
2. Tremblay, S. Height-to-Foot-Size Ratio. (n.d.). Retrived from https://www.livestrong.com/article/491821-height-to-foot-size-ratio/
3. Ordinary least squares. Wikipedia. (n.d.). Retrieved from https://en.wikipedia.org/wiki/Ordinary_least_squares
4. Fengjiutian88. How to understand OLS? (2018). Retrieved from http://www.360doc.com/content/18/0706/10/15930282_768242401.shtml 



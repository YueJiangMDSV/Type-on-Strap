---
layout: post
title: Simple Introduction to OLS
tags: [Blog, Archive]
excerpt_separator: <!--more-->
---

Today, in this blog post, I want to talk about a very useful regression method: Ordinary Least Squares (OLS). It is frequently used in economics, statistics, and data science. It is simple to use, but it is also the foundation of some of the more advanced regression methods. 
<!--more-->
So you may ask: what is ordinary least squares? What is a regression? And why am I learning this? What am I gonna use it for? Just bear with me, and I will provide examples of why regressions are very useful in today’s world. Also, because this blog is just an elementary introduction for ordinary least squares, so I will not dive into the difficult math equations to prove its assumptions and merits. Nor will I go over specific codes to calculate it. It is beyond the scope of this blog.

## What is regression?
You can find a very technical and complicated explanation on Wikipedia: [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)$$^1$$. 
Let me use a simple example to illustrate it. We often see in crime/detective movies, when the detectives see the footprints of a suspect, they can instantly tell roughly how tall the suspect is. How did they do that? Is that some kind of magic? Of course not! They know that because there is a relationship between a person’s height and foot size. According to a study by the University of Rhode Island Department of Electrical, Computer and Biomedical Engineering, the normal height-to-foot ratio is about 6.6:1$$^2$$.
    
In layman's terms, regression analysis is a method to study the relationship between variables. It can be the relationship between a person’s height and foot size (like the example above), or the relationship between the amount of sunlight and corn production, or the relationship between the amount of cheese your mom put in your mac & cheese and how delicious it is. Just like people are connected with different relationships, variables can also be connected with certain relationships. Regression analysis helps us discover these relationships. 
     
There are two important concepts in regression: 
1. Dependent variable, or often called response variable. It is the target that you want to study. You want to study the factors that affect the variable and even to predict the behavior of this variable. 
2. Independent variables, or often called explanatory variables. As the name suggests, these variables are the factors that explain the changes of the dependent variable. 
   
The key of regression analysis is to identify the right dependent variable, the right independent variables that impact the dependent variable, and the right way to illustrate this relationship. In the real world setting, there could be various kinds of relationships, such as linear, quadratic, polynomial, exponential, logarithmic, and other non-linear types. Here I will only introduce ordinary least squares, which is the basic linear regression form. 

## What is OLS?

You can refer to Wikipedia for a technical definition: [Ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)$$^3$$.

To put it in simple terms, it is a type of linear regression to estimate the unknown parameters in the linear regression model. It uses a optimization method that minimizes the sum of the squares of the differences between the observed dependent variables and the predicted dependent variables. 

The the OLS relation can be defined using this equation:

$$y_i=\beta_0+\beta_1x_i + \epsilon_i$$,

where $$\beta_0$$ and $$\beta_1x$$ are the unknown parameters we want to estimate, and $$\epsilon_i$$ is the error term, and we will not talk about it today. 

If we just look at $$y_i=\beta_0+\beta_1x_i$$, does this equation remind you of something similar? That's right! It looks just like the equation of a line. I think most of you have encountered such a function during high school. Back then, you probably did a lot of calculations to figure out the slopes and the intercepts of this kind of function. And this is what we are doing now! OLS is just a more complicated way to figure out the slopes and the intercepts. Behind this equation, mathematicians and statisticians did a lot of hard work to figure out how OLS works to calculate $$\beta_0$$ and $$\beta_1$$, and they also proved that why OLS does a very good job estimating $$\beta_0$$ and $$\beta_1$$.

The equation above only includes one explanatory variable, but in theory, we can add as many as we like. However, adding more variables does not always make the regression better. There are trade-offs and the details are beyond the scope of this post. 
    
Therea are two key components in the definition above: linear, and minimizing the sum of the squares of the differences. Let's first look at "linear".
   
### Linear
the relationship between a dependent variable and independent variables must be linear. Below is an example of a linear relationship$$^4$$

![]({{ "/assets/img/post_img/linear-2.png" | relative_url}})
   
We can see that as `x` increases, `y` aslo increases, and their relationship follows a somewhat straight line. 

An example of non-linear relationship can be seen from the graph below:

![]({{ "/assets/img/post_img/non-linear.png" | relative_url}})

In the graph above, we can see that first as `x` increases, `y` decreases. But then as `x` continues to increase, `y` increases. This is clearly not a linear relationship. It could be quadratic, but we cannot say for sure, since this is only part of the data. If the relationship is quadratic or in other forms, then we cannot use OLS. It would do a bad job estimating the relationship.

Thus, to apply OLS, you have to make sure the relationship between the response variable and the explanatory variables is linear. 

### Minimizing the sum of the squares of the differences.
The key for OLS to work so well in dipicitng the relationship between `x` and `y` is because the method it uses to estimate $$\beta_0$$ and $$\beta_1$$. This optimization method minimizes the sum of the squares of the differences. 

First, the differences. What differences?     

Let’s see the graph below. We can see in the graph that there is a line $$f(x_i)$$ almost simulates the pattern of $$y_1, y_2, …, y_{10}$$, but each $$y_i$$ is not exactly on the line. Here, the $$\{y_i\}$$ represents the true values, values that you get from a dataset. And the line $$f(x_i)$$ is the predicted value of $$\{y_i\}$$. It is a line that you think represents the relationship between $$x_i$$ and $$y_i$$. But how do we know this line is good? What does this line have to do with OLS?   

![]({{ "/assets/img/post_img/ols.png" | relative_url}})

      
Here, the differences are the differences between each $$y_i$$ and each $$f(x_i)$$, namely, the differences between each true value and each predicted value. To check how accurately each $$f(x_i)$$ predicts each $$y_i$$, we need to sum up all the differences. But some of the $$y_i$$ is larger than $$f(x_i)$$, some smaller. The positive and negative differences may cancel each other out. What shall we do then? To solve this problem, OLS first squares the differences, and then sums up the squared differences together. In this way, there are no negative differences to affect our analysis. 
   
This sum of differences can be expressed as:  
  
$$\sum (\beta_0+\beta_1x_i-y_i)^2$$
     
The job of OLS is to find the $$\beta_0$$ and $$\beta_1$$ that minimize this function.    
    
Sounds simple, right? But the math calculations behind it are not simple at all. It is very difficult for people to calculate the answers by hand. Fortunately, now we have a lot of computer software programs to do the math for us, such as Stata, Matlab, Python, and R. The calculations are beyond the scope of this introductory blog post and anyone interested can search for answers themselves.

## Final Words

Today this blog post introduces a regression method called ordinary least squares. It finds the $$\beta_0$$ and $$\beta_1$$ that can minimize the function:   

$$\sum (\beta_0+\beta_1x_i-y_i)^2$$

It looks easy, right? So does it really do a good job estimating the relationship between $$x_i$$ and $$y_i$$? 

Yes, of course!    
1. The estimator $$\beta_0$$ and $$\beta_1$$ are unbiased.
2. $$\beta_0$$ and $$\beta_1$$ are consistent.
3. $$\beta_0$$ and $$\beta_1$$ are asymptotically normal.    
    
The above terms may look difficult to understand, but in short, they mean that $$\beta_0$$ and $$\beta_1$$ calculated by OLS are almost “equal” to the true value.
   
Then you may ask: if OLS is so powerful, can I use it under any condition? 

NO!    

Even though OLS is powerful, the data has to meet certain conditions before you can apply OLS, and of course, OLS also has its limitations. 

Now that you learned what regression analysis is, go and try it for yourself and see what questions or problems you have. Or you can just go and tell your friend, you know how tall he or she is, by simply measuring his or her foot size! 

   
## References:
1. Regression analysis. Wikipedia. (n.d.). Retrieved from https://en.wikipedia.org/wiki/Regression_analysis  
2. Tremblay, S. Height-to-Foot-Size Ratio. (n.d.). Retrived from https://www.livestrong.com/article/491821-height-to-foot-size-ratio/
3. Ordinary least squares. Wikipedia. (n.d.). Retrieved from https://en.wikipedia.org/wiki/Ordinary_least_squares

### Note
This blog post is also part of the [MDS](https://masterdatascience.ubc.ca/) program curriculum. Thank you to the MDS teaching team of MDS for offering us opportunities to practice communication of data science topics to general audiences without data science backgrounds. 

